# -*- coding: utf-8 -*-
"""interactive-model-prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fuKPIPN9atEiLcS_ArapMx1hDp6BzBhs
"""

# @title 2. Load Model and Tokenizer
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
import warnings
warnings.filterwarnings('ignore')

def main():
    # Input parameters
    model_name = "HuggingFaceTB/SmolLM2-1.7B" #@param {type:"string"}

    print(f"Loading tokenizer for {model_name}...")
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)

    # Some models don't have a pad token, so we set it to eos_token
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    print("Tokenizer loaded.")

    print(f"Loading model {model_name}...")
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.float32,  # Use float32 for CPU
        device_map='cpu',
        trust_remote_code=True
    )

    print("Model loaded successfully!")

    # @title 3. Tokenize Input and Get Model Predictions
    import torch.nn.functional as F

    # Input parameters
    input_phrase = "The capital of France is" #@param {type:"string"}
    top_k_tokens = 5 #@param {type:"integer"}

    print(f"Input phrase: '{input_phrase}'")
    print("-" * 50)

    # Tokenize the input phrase
    input_ids = tokenizer.encode(input_phrase, return_tensors="pt")

    print(f"\n1. TOKENIZATION:")
    print(f"Token IDs: {input_ids[0].tolist()}")

    # Decode each token individually to show the breakdown
    tokens = []
    for i, token_id in enumerate(input_ids[0]):
        token = tokenizer.decode([token_id.item()])
        tokens.append(token)
        print(f"   Token {i+1}: ID={token_id.item():6d} â†’ '{token}'")

    print("-" * 50)

    # Get model predictions
    print(f"\n2. MODEL PREDICTIONS:")
    with torch.no_grad():
        outputs = model(input_ids)
        logits = outputs.logits[0, -1, :]  # Get logits for the last position

    # Convert logits to probabilities
    probabilities = F.softmax(logits, dim=-1)

    # Get top-k predictions
    top_k_probs, top_k_indices = torch.topk(probabilities, top_k_tokens)

    print(f"\nTop {top_k_tokens} predicted next words:")
    print(f"{'Rank':<6} {'Word':<20} {'Probability':<12} {'Token ID':<10}")
    print("-" * 58)

    for i in range(top_k_tokens):
        token_id = top_k_indices[i].item()
        prob = top_k_probs[i].item()

        # Decode the predicted token
        predicted_word = tokenizer.decode([token_id], skip_special_tokens=True)

        # Clean up the output (remove extra spaces if any)
        predicted_word = predicted_word.strip()

        print(f"{i+1:<6} {predicted_word:<20} {prob:>6.2%} ({prob:.4f}) {token_id:<10}")

    print("-" * 50)

    # Show the complete sentence with the top prediction
    if top_k_tokens > 0:
        best_token_id = top_k_indices[0].item()
        best_word = tokenizer.decode([best_token_id], skip_special_tokens=True).strip()
        print(f"\n3. COMPLETE SENTENCE WITH TOP PREDICTION:")
        print(f"'{input_phrase} {best_word}'")

if __name__ == "__main__":
    main()
